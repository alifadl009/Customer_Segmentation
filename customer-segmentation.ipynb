{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Customer Segmentation Project","metadata":{"editable":false}},{"cell_type":"markdown","source":"### Introduction","metadata":{"editable":false}},{"cell_type":"markdown","source":"\nCustomer segmentation is a powerful technique used in the field of marketing to divide a customer base into distinct groups or segments based on shared characteristics, behaviors, and preferences. These segments enable businesses to gain valuable insights into their customers and tailor their strategies to effectively target each group.\n\nIn this project, we will explore customer segmentation in the online retailer. The project includes a structured approach, starting with data loading and exploration, followed by data preprocessing, exploratory data analysis, customer segmentation techniques, segment profiling, and interpretation. Throughout the process, we will utilize Python libraries such as pandas, numpy, and scikit-learn for data manipulation, analysis, and modeling tasks.","metadata":{"editable":false}},{"cell_type":"markdown","source":"This project c is based on the \"Customer Segmentation Dataset\" by **Yasser H.**, which can be found on Kaggle. The dataset contains information about customer transactions for Online Retail Store.\n\nYou can access the dataset at [Kaggle - Customer Segmentation Dataset](https://www.kaggle.com/datasets/yasserh/customer-segmentation-dataset).","metadata":{"editable":false}},{"cell_type":"markdown","source":"### Libraries:\nImport necessary libraries and modules:  \n**Data manipulation**:\n* `Pandas`\n* `NumPy`\n  \n**Visualisation**:\n* `Matplotlib`\n* `Seaborn`\n* `Missingno`  \n\n**Machine Learning and preprocssing**:  \n* `Scikit-learn`  \n","metadata":{"editable":false}},{"cell_type":"code","source":"import os\nimport warnings\nimport numpy as np \nimport pandas as pd\nimport matplotlib.pyplot as plt \nimport seaborn as sns\nimport missingno as no\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.cluster import KMeans\nfrom sklearn.metrics import silhouette_score\n\n# Ignore warning messages\nwarnings.filterwarnings('ignore')","metadata":{"execution":{"iopub.status.busy":"2023-07-20T01:48:10.578384Z","iopub.execute_input":"2023-07-20T01:48:10.579201Z","iopub.status.idle":"2023-07-20T01:48:12.913834Z","shell.execute_reply.started":"2023-07-20T01:48:10.579150Z","shell.execute_reply":"2023-07-20T01:48:12.912369Z"},"editable":false,"trusted":true},"execution_count":1,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.23.5\n  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n","output_type":"stream"}]},{"cell_type":"code","source":"# Iterate through the '/kaggle/input' directory and its subdirectories\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        # Print the full file path of each file\n        print(os.path.join(dirname, filename))","metadata":{"execution":{"iopub.status.busy":"2023-07-20T01:48:12.915499Z","iopub.execute_input":"2023-07-20T01:48:12.915858Z","iopub.status.idle":"2023-07-20T01:48:12.925881Z","shell.execute_reply.started":"2023-07-20T01:48:12.915828Z","shell.execute_reply":"2023-07-20T01:48:12.924618Z"},"editable":false,"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"/kaggle/input/customer-segmentation-dataset/Online Retail.xlsx\n","output_type":"stream"}]},{"cell_type":"markdown","source":"### Load Data","metadata":{"editable":false}},{"cell_type":"code","source":"data  = pd.read_excel('/kaggle/input/customer-segmentation-dataset/Online Retail.xlsx', sheet_name='Online Retail')","metadata":{"editable":false,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Explortary Data Analysis (EDA)  \nEDA techniques are applied to gain insights into the dataset, identify patterns, and understand the distribution of variables.","metadata":{"editable":false}},{"cell_type":"code","source":"data.head(5)","metadata":{"editable":false,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data.info()","metadata":{"editable":false,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data.isna().sum()","metadata":{"editable":false,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"no.bar(data)\nplt.show()","metadata":{"editable":false,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Calculate the percentage of missing data in the CustomerID column\nprint(f'Percentage of missing data from CustomerID column is: {round(data.CustomerID.isna().sum() / data.shape[0] *100,2)}%')","metadata":{"editable":false,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Missing values are detected, primarily in the `CustomerID` column and `Description` column. The percentage of missing data in the CustomerID column is calculated, revealing the extent of the data quality issue.","metadata":{"editable":false}},{"cell_type":"markdown","source":"\nBefore removing null values from the 'Description' column, we can replace those null values with a placeholder value to indicate that the description information is missing. This step ensures that we retain the information about missing descriptions and can still use it in our analysis.","metadata":{"editable":false}},{"cell_type":"code","source":"data['Description'].fillna('No Description', inplace=True)","metadata":{"editable":false,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data['Description'].isna().sum()","metadata":{"editable":false,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Let's remove of any null values that may be present within the dataset.","metadata":{"editable":false}},{"cell_type":"code","source":"df = data.dropna()","metadata":{"editable":false,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.isna().sum()","metadata":{"editable":false,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.shape","metadata":{"editable":false,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.describe()","metadata":{"editable":false,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We notice that there are negative values in the `Quantity` column and zero value in `UnitPrice` column . let's assume that we consulted stakeholders and they confirmed that these negative values are a result of typographical errors, and the correct values should be positive. As a result, we will consider only the positive values in the `Quantity` column for our analysis and further processing. And they have confirmed that the zero `UnitPrice` values are invalid entries or represent missing data. As per their guidance, we will proceed by removing these records from the dataset.","metadata":{"editable":false}},{"cell_type":"code","source":"df['Quantity'] = df['Quantity'].abs()","metadata":{"editable":false,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.drop(df[df['UnitPrice'] == 0].index, axis=0, inplace=True)","metadata":{"editable":false,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Feature Engineering","metadata":{"editable":false}},{"cell_type":"markdown","source":"Now that the dataset appears to be in good condition, our focus is shifting towards customer segmentation. In order to accomplish this, we will transform the current sales data into a customer-level perspective.","metadata":{"editable":false}},{"cell_type":"markdown","source":"we will use the RFM marketing Metrics, to look at customer's behaviours based on:  \n* **Recency**: How many days had passed since customer last purchased.\n* **Frequency**: - How many times a customer had shopped here.\n* **Monetary Value**: - How much money had the customer spent.","metadata":{"editable":false}},{"cell_type":"markdown","source":"![RFM](https://d35fo82fjcw0y8.cloudfront.net/2018/03/01013508/Incontent_image.png)","metadata":{"editable":false}},{"cell_type":"markdown","source":"Calculate the total price for each transaction: This step creates a new column named `TotalPrice` by multiplying the `Quantity` and `UnitPrice` columns. It calculates the total monetary value for each transaction, which can be useful for customer segmentation.","metadata":{"editable":false}},{"cell_type":"code","source":"df['TotalPrice'] = df['Quantity'] * df['UnitPrice']","metadata":{"editable":false,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Find the most recent date in the dataset: Identifying the most recent date in the `InvoiceDate` column helps in calculating the recency of customer transactions.","metadata":{"editable":false}},{"cell_type":"markdown","source":"##### **Why we didn't use `Country` column?**  \nWhen dealing with categorical variables like the `Country` column in a dataset with a large number of unique categories (37 countries in this case), it is essential to consider the appropriate approach for handling such data.  \nUsing the `Country` column directly as a feature in clustering algorithms like K-means might not be the best approach because it could lead to a high-dimensional feature space and potential inefficiencies in the clustering process. The large number of unique categories may introduce noise and make it challenging for the algorithm to identify meaningful patterns.\nIf you believe that the `Country` information is essential for segmentation, you could consider aggregating countries into broader regions or continents. This way, you reduce the number of categories while still capturing regional trends.  ","metadata":{"editable":false}},{"cell_type":"markdown","source":"  Group the data by `CustomerID` and calculate Recency, Frequency, and Monetary values: This step groups the data by `CustomerID` and calculates three important metrics : recency (the number of days between the most recent date and the maximum `InvoiceDate` for each customer), frequency (the count of invoices for each customer), and monetary value (the sum of the `TotalPrice` for each customer).","metadata":{"editable":false}},{"cell_type":"code","source":"most_recent_date = df['InvoiceDate'].max()\ncustomer_df = df.groupby('CustomerID').agg({'InvoiceDate': lambda x: (most_recent_date - x.max()).days,\n                                            'InvoiceNo': 'count',\n                                            'TotalPrice': 'sum'})\ncustomer_df.rename(columns={'InvoiceDate':'Recency', 'InvoiceNo':'Frequency', 'TotalPrice':'Monetary'}, inplace=True)\ncustomer_df.head(5)","metadata":{"editable":false,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"customer_df.shape","metadata":{"editable":false,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"customer_df.describe()","metadata":{"editable":false,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sns.pairplot(customer_df)","metadata":{"editable":false,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Standardize the customer dataframe using StandardScaler: Standardizing the data using StandardScaler ensures that all the variables are on the same scale, which is necessary for K-means clustering.","metadata":{"editable":false}},{"cell_type":"code","source":"scaler = StandardScaler()\nnorm_df = scaler.fit_transform(customer_df)\nnorm_df","metadata":{"editable":false,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Model Building","metadata":{"editable":false}},{"cell_type":"markdown","source":"Performs K-means clustering with different numbers of clusters (ranging from 1 to 10)","metadata":{"editable":false}},{"cell_type":"code","source":"inertia = []\nfor k in range(1,11):\n    kmeans = KMeans(n_clusters=k, random_state=42)\n    kmeans.fit(norm_df)\n    inertia.append(kmeans.inertia_)","metadata":{"editable":false,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Plot the inertia values ","metadata":{"editable":false}},{"cell_type":"code","source":"plt.plot(range(1,11), \n         inertia,\n         marker='o')\nplt.xlabel('Number of Cluster')\nplt.ylabel('Inertia')","metadata":{"editable":false,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"K-means clustering is performed with different numbers of clusters, and we think `3` is the most appropriate value for number of clusters.","metadata":{"editable":false}},{"cell_type":"markdown","source":"Lets validate our findings using silhouette score","metadata":{"editable":false}},{"cell_type":"code","source":"sil = []\nfor k in range(2,11):\n    kmeans = KMeans(n_clusters=k, random_state=42)\n    kmeans.fit(norm_df)\n    sil.append(silhouette_score(norm_df, kmeans.labels_))","metadata":{"editable":false,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\nplt.plot(range(2,11), sil, marker='o')\nplt.xlabel(\"Number of Clusters\")\nplt.ylabel(\"Silhouette Score\")\nplt.show()","metadata":{"editable":false,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The silhouette scores for 3, 4, and 5 clusters were approximately similar. However, the inertia value significantly decreased from 3 to 4 clusters and  relatively less when moving from 4 to 5 clusters. Consequently, we selected 3 clusters as the optimal number for customer segmentation.","metadata":{"editable":false}},{"cell_type":"markdown","source":"### Further Analysis and Interpretation","metadata":{"editable":false}},{"cell_type":"markdown","source":"Analyze The segmented customer data in more detail to understand the characteristics and behaviors of each customer segment.","metadata":{"editable":false}},{"cell_type":"markdown","source":"Perform K-means clustering with the final number of clusters","metadata":{"editable":false}},{"cell_type":"code","source":"final_kmeans = KMeans(n_clusters=3,random_state=42)\nfinal_kmeans.fit(norm_df)\n","metadata":{"editable":false,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Create a new dataframe with customer information and assigned clusters","metadata":{"editable":false}},{"cell_type":"code","source":"final_df = pd.DataFrame(customer_df, columns=customer_df.columns, index=customer_df.index)\nfinal_df['Cluster'] = final_kmeans.labels_ + 1 # I want to have cluster labels starting from 1 instead of 0\nfinal_df.head(10)","metadata":{"editable":false,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Visualize the distribution of clusters ","metadata":{"editable":false}},{"cell_type":"code","source":"sns.histplot(final_df.Cluster)\nplt.xticks(range(1,4))\nplt.show()","metadata":{"editable":false,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"final_df.groupby('Cluster').agg({'Monetary':'mean',\n                                 'Frequency':'mean',\n                                 'Recency':'mean'})","metadata":{"editable":false,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"After segmenting the customer data into three distinct clusters, we conducted a detailed analysis to comprehend the unique characteristics and behaviors exhibited by each group. The segmentation revealed clear patterns in terms of customer recency, purchase frequency, and monetary value.  \n* **Cluster 1: \"High-Value Regular Customers\"**:  \nThis cluster consists of customers with relatively high monetary value, moderate frequency, and recent transactions. They are likely to be loyal and valuable customers who make regular purchases.  \n\n* **Cluster 2: \"Low-Value Occasional Customers**\"\nThis cluster includes customers with lower monetary value, lower frequency, and higher recency. They might be occasional buyers who make infrequent purchases.  \n\n* **Cluster 3: \"High-Value VIP Customers**\"  \nThis cluster represents customers with exceptionally high monetary value, high frequency, and very recent transactions. They are top-tier customers who contribute significantly to the business's revenue and should be treated as VIPs.","metadata":{"editable":false}},{"cell_type":"markdown","source":"It's important to note that this project is a starting point, and further analyses and experiments can be performed to gain deeper insights and refine the segmentation strategy. Additionally, business-specific factors and domain knowledge should be taken into account when interpreting the results and making strategic decisions based on the customer segments.","metadata":{"editable":false}}]}